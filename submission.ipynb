{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load Training Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import random, os, time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_discrete(s):\n",
    "    if len(s) == 1:\n",
    "        return s[0]\n",
    "    return random.choice(s)\n",
    "\n",
    "rz_dict = {'bilinear': Image.BILINEAR,\n",
    "           'bicubic': Image.BICUBIC,\n",
    "           'lanczos': Image.LANCZOS,\n",
    "           'nearest': Image.NEAREST}\n",
    "\n",
    "def custom_resize(img, opt):\n",
    "    interp = sample_discrete(opt.rz_interp)\n",
    "    return TF.resize(img, opt.loadSize, interpolation=rz_dict[interp])\n",
    "\n",
    "def dataset_folder(opt, root):\n",
    "    if opt.isTrain:\n",
    "        crop_func = transforms.RandomCrop(opt.cropSize)\n",
    "    elif opt.no_crop:\n",
    "        crop_func = transforms.Lambda(lambda img: img)\n",
    "    else:\n",
    "        crop_func = transforms.CenterCrop(opt.cropSize)\n",
    "\n",
    "    if opt.isTrain and not opt.no_flip:\n",
    "        flip_func = transforms.RandomHorizontalFlip()\n",
    "    else:\n",
    "        flip_func = transforms.Lambda(lambda img: img)\n",
    "    if not opt.isTrain and opt.no_resize:\n",
    "        rz_func = transforms.Lambda(lambda img: img)\n",
    "    else:\n",
    "        rz_func = transforms.Lambda(lambda img: custom_resize(img, opt))\n",
    "\n",
    "    dset = datasets.ImageFolder(\n",
    "            root,\n",
    "            transforms.Compose([\n",
    "                rz_func,\n",
    "                transforms.Lambda(lambda img: Image.fromarray(np.array(img))),\n",
    "                crop_func,\n",
    "                flip_func,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]))\n",
    "    return dset\n",
    "\n",
    "def get_dataset(opt):\n",
    "    dset_lst = []\n",
    "    for cls in opt.classes:\n",
    "        root = opt.dataroot + '/' + cls\n",
    "        dset = dataset_folder(opt, root)\n",
    "        dset_lst.append(dset)\n",
    "    return torch.utils.data.ConcatDataset(dset_lst)\n",
    "\n",
    "def create_dataloader(opt):\n",
    "    shuffle = not opt.serial_batches if (opt.isTrain and not opt.class_bal) else False\n",
    "    dataset = get_dataset(opt)\n",
    "    # sampler = get_bal_sampler(dataset) if opt.class_bal else None\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=opt.batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              #sampler=sampler,\n",
    "                                              )\n",
    "    return data_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader = create_dataloader(opt)\n",
    "dataset_size = len(data_loader)\n",
    "print('#training images = %d' % dataset_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "model_urls = {\n",
    "    # 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    # 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    # 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    # 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module): # 单独写出来没啥用，最好整合至trainer内\n",
    "    def __init__(self, opt):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.total_steps = 0\n",
    "        self.isTrain = opt.isTrain\n",
    "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
    "        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n",
    "\n",
    "    def save_networks(self, epoch):\n",
    "        save_filename = 'model_epoch_%s.pth' % epoch\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "\n",
    "        # serialize model and optimizer to dict\n",
    "        state_dict = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer' : self.optimizer.state_dict(),\n",
    "            'total_steps' : self.total_steps,\n",
    "        }\n",
    "\n",
    "        torch.save(state_dict, save_path)\n",
    "\n",
    "    # load models from the disk\n",
    "    def load_networks(self, epoch):\n",
    "        load_filename = 'model_epoch_%s.pth' % epoch\n",
    "        load_path = os.path.join(self.save_dir, load_filename)\n",
    "\n",
    "        print('loading the model from %s' % load_path)\n",
    "        # if you are using PyTorch newer than 0.4 (e.g., built from\n",
    "        # GitHub source), you can remove str() on self.device\n",
    "        state_dict = torch.load(load_path, map_location=self.device)\n",
    "        if hasattr(state_dict, '_metadata'):\n",
    "            del state_dict._metadata\n",
    "\n",
    "        self.model.load_state_dict(state_dict['model'])\n",
    "        self.total_steps = state_dict['total_steps']\n",
    "\n",
    "        if self.isTrain and not self.opt.new_optim:\n",
    "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "            ### move optimizer state to GPU\n",
    "            for state in self.optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.to(self.device)\n",
    "\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = self.opt.lr\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def test(self):\n",
    "        with torch.no_grad():\n",
    "            self.forward()\n",
    "\n",
    "class Trainer(BaseModel):\n",
    "    def name(self):\n",
    "        return 'Trainer'\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        super(Trainer, self).__init__(opt)\n",
    "\n",
    "        if self.isTrain and not opt.continue_train:\n",
    "            self.model = resnet50(pretrained=True)\n",
    "            self.model.fc = nn.Linear(2048, 1)\n",
    "            torch.nn.init.normal_(self.model.fc.weight.data, 0.0, opt.init_gain)\n",
    "\n",
    "        if not self.isTrain or opt.continue_train:\n",
    "            self.model = resnet50(num_classes=1)\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "            # initialize optimizers\n",
    "            if opt.optim == 'adam':\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                                  lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            elif opt.optim == 'sgd':\n",
    "                self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                                 lr=opt.lr, momentum=0.0, weight_decay=0)\n",
    "            else:\n",
    "                raise ValueError(\"optim should be [adam, sgd]\")\n",
    "\n",
    "        if not self.isTrain or opt.continue_train:\n",
    "            self.load_networks(opt.epoch)\n",
    "        self.model.to(opt.gpu_ids[0])\n",
    "\n",
    "\n",
    "    def adjust_learning_rate(self, min_lr=1e-6):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] /= 10.\n",
    "            if param_group['lr'] < min_lr:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def set_input(self, input):\n",
    "        self.input = input[0].to(self.device)\n",
    "        self.label = input[1].to(self.device).float()\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = self.model(self.input)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss_fn(self.output.squeeze(1), self.label)\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.forward()\n",
    "        self.loss = self.loss_fn(self.output.squeeze(1), self.label)\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Trainer(opt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Early Stopping Function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=1, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.score_max = -np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, score, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.score_max:.6f} --> {score:.6f}).  Saving model ...')\n",
    "        model.save_networks('best')\n",
    "        self.score_max = score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=opt.earlystop_epoch, delta=-0.001, verbose=True) # default=5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training & Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "\n",
    "def get_val_opt():\n",
    "    val_opt = TrainOptions().parse(print_options=False)\n",
    "    val_opt.dataroot = '{}/{}/'.format(val_opt.dataroot, val_opt.val_split)\n",
    "    val_opt.isTrain = False\n",
    "    val_opt.no_resize = False\n",
    "    val_opt.no_crop = False\n",
    "    val_opt.serial_batches = True\n",
    "    val_opt.jpg_method = ['pil']\n",
    "    if len(val_opt.blur_sig) == 2:\n",
    "        b_sig = val_opt.blur_sig\n",
    "        val_opt.blur_sig = [(b_sig[0] + b_sig[1]) / 2]\n",
    "    if len(val_opt.jpg_qual) != 1:\n",
    "        j_qual = val_opt.jpg_qual\n",
    "        val_opt.jpg_qual = [int((j_qual[0] + j_qual[-1]) / 2)]\n",
    "\n",
    "    return val_opt\n",
    "\n",
    "def validate(model, opt):\n",
    "    data_loader = create_dataloader(opt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_true, y_pred = [], []\n",
    "        for img, label in data_loader:\n",
    "            in_tens = img.cuda()\n",
    "            y_pred.extend(model(in_tens).sigmoid().flatten().tolist())\n",
    "            y_true.extend(label.flatten().tolist())\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    r_acc = accuracy_score(y_true[y_true==0], y_pred[y_true==0] > 0.5)\n",
    "    f_acc = accuracy_score(y_true[y_true==1], y_pred[y_true==1] > 0.5)\n",
    "    acc = accuracy_score(y_true, y_pred > 0.5)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    return acc, ap, r_acc, f_acc, y_true, y_pred\n",
    "\n",
    "val_opt = get_val_opt()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(opt.niter)): # 100\n",
    "    epoch_start_time = time.time()\n",
    "    iter_data_time = time.time()\n",
    "    epoch_iter = 0\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        model.total_steps += 1\n",
    "        epoch_iter += opt.batch_size\n",
    "\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        if model.total_steps % opt.loss_freq == 0:\n",
    "            print(\"Train loss: {} at step: {}\".format(model.loss, model.total_steps))\n",
    "\n",
    "        if model.total_steps % opt.save_latest_freq == 0:\n",
    "            print('saving the latest model %s (epoch %d, model.total_steps %d)' %\n",
    "                  (opt.name, epoch, model.total_steps))\n",
    "            model.save_networks('latest')\n",
    "\n",
    "        # print(\"Iter time: %d sec\" % (time.time()-iter_data_time))\n",
    "        # iter_data_time = time.time()\n",
    "\n",
    "    if epoch % opt.save_epoch_freq == 0:\n",
    "        print('saving the model at the end of epoch %d, iters %d' %\n",
    "              (epoch, model.total_steps))\n",
    "        model.save_networks('latest')\n",
    "        model.save_networks(epoch)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    acc, ap = validate(model.model, val_opt)[:2]\n",
    "    print(\"(Val @ epoch {}) acc: {}; ap: {}\".format(epoch, acc, ap))\n",
    "\n",
    "    early_stopping(acc, model)\n",
    "    if early_stopping.early_stop:\n",
    "        cont_train = model.adjust_learning_rate()\n",
    "        if cont_train:\n",
    "            print(\"Learning rate dropped by 10, continue training...\")\n",
    "            early_stopping = EarlyStopping(patience=opt.earlystop_epoch, delta=-0.002, verbose=True)\n",
    "        else:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "    model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}